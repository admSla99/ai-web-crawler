name: Weekly Web Crawler

on:
  schedule:
    # Run every Sunday at 23:59 UTC
    - cron: '59 23 * * 0'
  # Allow manual triggering for testing
  workflow_dispatch:

jobs:
  crawl-and-send:
    runs-on: ubuntu-latest

    env:
      N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install

      - name: Run crawler
        run: |
          python crawler.py
          echo "Crawler execution completed"

      - name: Install requests package for webhook
        run: |
          pip install requests

      - name: Send data to n8n webhook
        if: env.N8N_WEBHOOK_URL != ''
        run: |
          echo "Sending data to n8n webhook..."
          python send_to_n8n.py ai_training_data
          echo "Data sent to n8n webhook"

      - name: Upload crawled data as artifact
        uses: actions/upload-artifact@v3
        with:
          name: crawled-data
          path: ai_training_data/
          retention-days: 7
